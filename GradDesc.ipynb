{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the gradient descent:\n",
      "2.210739197207331e-06\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# This class has the following methods:\n",
    "# gradient_descendent: calculates the gradient of a function f(x) = 2x\n",
    "# gradient_descendent_ssr: calculates the gradient of the sum of squared residuals\n",
    "# sgd: calculates the stochastic gradient descent\n",
    "\n",
    "class gradient_ml:\n",
    "    # This class has the following attributes:\n",
    "    # start: initial value of the variable\n",
    "    # lr: learning rate\n",
    "    # ni: number of iterations\n",
    "    # tol: tolerance\n",
    "    def __init__(self, start: list[float] | float, learn: int | float, n_iter: int | float, tolerance: int | float) -> float:\n",
    "        self.start = start if isinstance(start, list) else float(start)\n",
    "        self.lr = float(learn)\n",
    "        self.ni = int(n_iter)\n",
    "        self.tol = float(tolerance)\n",
    "    \n",
    "# This method calculates the gradient of the function f(x) = 2x\n",
    "    def gradient_descendent(self):\n",
    "        cache = {}\n",
    "        def gradient(x:int | float) -> float:\n",
    "            if x in cache:\n",
    "                return cache[x]\n",
    "            # This function calculates the gradient of the function f(x) = 2x\n",
    "            return 2*x\n",
    "        vector = self.start\n",
    "        for _ in range(self.ni):\n",
    "             # Compute the gradient\n",
    "            grad_value = gradient(vector)\n",
    "            cache[vector] = grad_value\n",
    "            # Compute the step\n",
    "            diff = -self.lr * grad_value\n",
    "            # Verify if the absolute difference is small enough\n",
    "            if np.abs(diff) <= self.tol:\n",
    "                break\n",
    "            # Update the value of the variable\n",
    "            vector += diff \n",
    "        return vector\n",
    "\n",
    "# This method calculates the gradient of the sum of squared residuals\n",
    "    def ssr_grad(self, x: float, y: float, b: float) -> float:\n",
    "        res = b[0] + b[1] * x - y\n",
    "        return np.array([res.mean(), (res * x).mean()])  # .mean() is a method of np.ndarray\n",
    "\n",
    "# This method calculates the gradient of the sum of squared residuals using the method ssr_grad\n",
    "    def gradient_descendent_ssr(self, x:float, y:float) -> float:\n",
    "        vector = np.array(self.start)\n",
    "        for _ in range(self.ni):\n",
    "            diff = - self.lr * np.array(self.ssr_grad(x, y, vector))\n",
    "            if np.all(np.abs(diff) <= self.tol):\n",
    "                break\n",
    "            vector += diff\n",
    "        return vector\n",
    "\n",
    "# This method calculates the stochastic gradient descent\n",
    "    def sgd(self, x:float, y:float, batch_size:float, dtype=\"float64\", random_state=None) -> float:\n",
    "        # Converting x and y to NumPy arrays\n",
    "        x, y = np.array(x, dtype=dtype), np.array(y, dtype=dtype)\n",
    "        n_obs = x.shape[0]\n",
    "        if n_obs != y.shape[0]:\n",
    "            raise ValueError(\"'x' and 'y' lengths do not match\")\n",
    "        xy = np.c_[x.reshape(n_obs, -1), y.reshape(n_obs, 1)]\n",
    "        \n",
    "        # Initializing the random number generator\n",
    "        seed = None if random_state is None else int(random_state)\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        # Initializing the vector\n",
    "        vector = np.array(self.start, dtype=dtype)\n",
    "        \n",
    "        # Performing the gradient descent loop\n",
    "        for _ in range(self.ni):\n",
    "            # Shuffle x and y\n",
    "            rng.shuffle(xy)\n",
    "\n",
    "        # Performing minibatch moves\n",
    "        for start in range(0, n_obs, batch_size):\n",
    "            stop = start + batch_size\n",
    "            x_batch, y_batch = xy[start:stop, :-1], xy[start:stop, -1:]\n",
    "\n",
    "            # Recalculating the difference\n",
    "            grad = np.array(self.ssr_grad(x_batch, y_batch, vector), dtype)\n",
    "            diff = -self.lr * grad\n",
    "\n",
    "            # Checking if the absolute difference is small enough\n",
    "            if np.all(np.abs(diff) <= self.tol):\n",
    "                break\n",
    "\n",
    "            # Updating the values of the variables\n",
    "            vector += diff\n",
    "\n",
    "        return vector if vector.shape else vector.item()\n",
    "    \n",
    "# Main function to test the class\n",
    "if __name__ == \"__main__\":\n",
    "    var = 1\n",
    "    if var == 1:\n",
    "        grad = gradient_ml(10.0, 0.2, 50, 1e-06)\n",
    "        result1 = grad.gradient_descendent()\n",
    "        print(\"This is the gradient descent:\")\n",
    "        print(result1)\n",
    "    elif var == 2:\n",
    "        grad = gradient_ml([10.0, 10.0], 0.0008, 100_000, 1e-06)\n",
    "        x = np.array([5, 15, 25, 35, 45, 55])\n",
    "        y = np.array([5, 20, 14, 32, 22, 38])\n",
    "        result2 = grad.gradient_descendent_ssr(x,y)\n",
    "        print(\"This is the gradient descent of the sum of squared residuals:\")\n",
    "        print(result2)\n",
    "    elif var == 3:\n",
    "        x = np.array([5, 25, 55, 75, 95, 115])\n",
    "        y = np.array([5, 30, 60, 70, 80, 100])\n",
    "        grad = gradient_ml([0.5, 0.5], 0.0008, 100_000, 1e-06)\n",
    "        result3 = grad.sgd(x, y, batch_size=3, random_state=0)\n",
    "        print(\"This is the stochastic gradient descent:\")\n",
    "        print(result3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.007128528\n",
      "2.0000508\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Create needed objects\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=0.1, momentum=0.9)\n",
    "var = tf.Variable(2.5)\n",
    "cost = lambda: 2 + var ** 2\n",
    "\n",
    "# Perform optimization\n",
    "for _ in range(100):\n",
    "    sgd.minimize(cost, var_list=[var])\n",
    "\n",
    "# Extract results\n",
    "var_value = var.numpy()\n",
    "print(var_value)\n",
    "\n",
    "costo = cost().numpy()\n",
    "print(costo)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sci-py",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
